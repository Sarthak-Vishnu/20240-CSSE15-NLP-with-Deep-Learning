{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3bc2da0a-7451-4b9a-bdc5-5582dbd545ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "90a3707f-751d-48fc-979d-01e4eb7c5238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0eb1b3-0603-46e8-954d-299c0bae9d09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b97a3bfa-220d-4571-b6ad-be006ae8e177",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at Rocketknight1/distilbert-base-uncased-finetuned-squad were not used when initializing TFDistilBertForQuestionAnswering: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForQuestionAnswering were not initialized from the model checkpoint at Rocketknight1/distilbert-base-uncased-finetuned-squad and are newly initialized: ['dropout_59']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"Rocketknight1/distilbert-base-uncased-finetuned-squad\"\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b299877f-2083-47f4-8bf2-dbc283bad46e",
   "metadata": {},
   "source": [
    "# Testing Que-Ans abilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83906a59-e8c5-42c2-98d9-1ae881892a71",
   "metadata": {},
   "source": [
    "### Example 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7bc2705e-04ff-4295-8e0f-00e242cce612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "65\n",
      "[ 3086 10595]\n",
      "\n",
      "\n",
      "Que: What kind of mechanisms is Transformer based on?\n",
      "Ans: attention mechanisms\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"The dominant sequence transduction models are based on complex recurrent or convolutional \n",
    "neural networks in an encoder-decoder configuration. The best performing models also connect the encoder\n",
    "and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, \n",
    "based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on \n",
    "two machine translation tasks show these models to be superior in quality while being more parallelizable \n",
    "and requiring significantly less time to train.\"\"\"\n",
    "\n",
    "question = \"What kind of mechanisms is Transformer based on?\"\n",
    "\n",
    "inputs = tokenizer([context], [question], return_tensors=\"np\")\n",
    "\n",
    "outputs = model(inputs)\n",
    "\n",
    "start_position = np.argmax(outputs.start_logits[0])\n",
    "end_position = np.argmax(outputs.end_logits[0])\n",
    "print(start_position)\n",
    "print(end_position)\n",
    "\n",
    "# Extract this substring from the inputs\n",
    "answer = inputs[\"input_ids\"][0, start_position: end_position + 1]\n",
    "print(answer)\n",
    "print(\"\\n\")\n",
    "print(\"Que:\", question)\n",
    "print(\"Ans:\", tokenizer.decode(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972ac7ed-8b41-4728-ba63-23ed08af260e",
   "metadata": {},
   "source": [
    "### Example 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "474a8c45-f648-4d39-8035-0e44b68274c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "70\n",
      "[2009 1005 1055 1037 5475 2173 1999 1996 2690 1997 1037 5697 2103]\n",
      "\n",
      "\n",
      "Que: Where is this Cafe?\n",
      "Ans: it's a calm place in the middle of a busy city\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"In a small cafe on a busy street, the smell of fresh coffee fills the air. \n",
    "People chat and laugh while soft music plays in the background. The baristas \n",
    "make each coffee with care, and everyone enjoys their time here, whether chatting \n",
    "with friends or enjoying a good book. It's a calm place in the middle of a \n",
    "busy city, where people can relax and enjoy simple moments.\"\"\"\n",
    "\n",
    "question = \"Where is this Cafe?\"\n",
    "\n",
    "inputs = tokenizer([context], [question], return_tensors=\"np\")\n",
    "\n",
    "outputs = model(inputs)\n",
    "\n",
    "start_position = np.argmax(outputs.start_logits[0])\n",
    "end_position = np.argmax(outputs.end_logits[0])\n",
    "print(start_position)\n",
    "print(end_position)\n",
    "\n",
    "# Extract this substring from the inputs\n",
    "answer = inputs[\"input_ids\"][0, start_position: end_position + 1]\n",
    "print(answer)\n",
    "print(\"\\n\")\n",
    "print(\"Que:\", question)\n",
    "print(\"Ans:\", tokenizer.decode(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1cf1a8-a01a-4b0a-8564-a1929c09dc3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
